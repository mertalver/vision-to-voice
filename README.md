# Voice-Based Image Description

A **multimodal AI system** that generates natural image descriptions
using **captioning, object detection, emotion recognition, and
text-to-speech (TTS)**.
Designed to assist visually impaired users, the system analyzes images
and provides spoken descriptions with clear, concise, and context-aware
details.

------------------------------------------------------------------------

## âœ¨ Features

-   ğŸ–¼ **Image Captioning** â†’ Generates an initial caption using
    BLIP.
-   ğŸ” **Object Detection** â†’ Identifies objects and their positions in
    the image (YOLOv5).
-   ğŸ™‚ **Emotion Recognition** â†’ Detects facial expressions and
    summarizes emotions.
-   ğŸ§  **Text Generation (LLM)** â†’ Combines captions, objects, and
    emotions to create a natural and accessible description using a Qwen-3
    model.
-   ğŸ”Š **Text-to-Speech** â†’ Converts the generated description into
    speech (gTTS).
-   âš¡ **Real-Time API** â†’ FastAPI-based REST API for easy
    integration.
-   ğŸ“‚ **Dataset Utilities** â†’ Scripts to download sample datasets
    (Flickr1K, COCO2017).
-   ğŸ”‘ **Customizable Models** â†’ Configurable paths for BLIP, YOLO,
    Qwen-3, and TTS models.

------------------------------------------------------------------------

## ğŸ›  System Architecture

1.  **Frontend** â†’ Simple HTML/JS interface for uploading images.
2.  **FastAPI Backend** (`main.py`) â†’ Routes requests and orchestrates
    services.
3.  **Services**:
    -   `image_captioning_service.py` â†’ BLIP-based
        captioning
    -   `object_detection_service.py` â†’ YOLO-based detection
    -   `emotion_detection_service.py` â†’ Facial emotion recognition
    -   `text_generation_service.py` â†’ LLM-powered description
        synthesis
    -   `tts_service.py` â†’ Converts description to audio
4.  **Endpoints** (`endpoints.py`) â†’ `/api/describe-image/` pipeline:
    -   Caption â†’ Object detection â†’ Emotion recognition â†’ LLM
        description â†’ TTS audio
5.  **Utilities**:
    -   `get_dataset.py` â†’ Downloads datasets for testing
    -   `config.py` â†’ Model paths & settings

------------------------------------------------------------------------

## ğŸ“‚ Project Structure

    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ main.py                     # FastAPI app
    â”‚   â”œâ”€â”€ config.py                   # Model paths & config
    â”‚   â”œâ”€â”€ endpoints.py                # API endpoints
    â”‚   â”œâ”€â”€ services/
    â”‚   â”‚   â”œâ”€â”€ image_captioning_service.py  # BLIP captioning
    â”‚   â”‚   â”œâ”€â”€ object_detection_service.py  # YOLO detection
    â”‚   â”‚   â”œâ”€â”€ emotion_detection_service.py # FER detection
    â”‚   â”‚   â”œâ”€â”€ text_generation_service.py   # Qwen text generation
    â”‚   â”‚   â””â”€â”€ tts_service.py               # gTTS speech
    â”œâ”€â”€ get_dataset.py                  # Dataset downloader
    â”œâ”€â”€ requirements.txt                # Dependencies
    â”œâ”€â”€ README.md                       # Documentation

------------------------------------------------------------------------

## âš™ï¸ Installation & Usage

### Prerequisites

-   Python 3.9+
-   PyTorch & Transformers
-   FastAPI & Uvicorn
-   YOLOv5 weights (`yolov5s.pt`)
-   BLIP model (e.g., `Salesforce/blip-image-captioning-base`)
-   Qwen-3 LLM model (`.gguf` format, configured in `config.py`)

### Setup

1.  Clone repository:

    ``` bash
    git clone https://github.com/yourusername/voice-based-image-description.git
    cd voice-based-image-description
    ```

2.  Create environment and install dependencies:

    ``` bash
    python -m venv venv
    source venv/bin/activate   # Linux/Mac
    venvScriptsactivate    # Windows

    pip install -r requirements.txt
    ```

3.  Start API server:

    ``` bash
    uvicorn app.main:app --reload
    ```

4.  Access frontend at `http://localhost:8000` (index.html).

------------------------------------------------------------------------

## ğŸ® Example API Usage

### Describe Image

``` http
POST /api/describe-image/
Content-Type: multipart/form-data
file: <image.jpg>
```

**Response** â†’ Audio stream (`audio/wav`) with spoken description.

Example pipeline:
1. Upload an image with multiple people.
2. System generates a caption, detects objects, counts people,
recognizes faces, and summarizes emotions.
3. Qwen-3 model produces a clear **Turkish description** suitable for
visually impaired users.
4. gTTS converts description to speech and returns audio.

------------------------------------------------------------------------
